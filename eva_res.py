# pip install google-generativeai python-dotenv
from typing import List, Dict, Any, Optional
import time
import json
import google.generativeai as genai
import os
from dotenv import load_dotenv  # Import the library

load_dotenv()  # Load variables from .env file

# --- Configuration ---

google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    print("Error: GOOGLE_API_KEY environment variable not set.")
    print("Please set it before running the script.")
    # In a real application, you might raise an exception or handle this differently
    # For this script, we'll allow it to proceed but evaluation will fail
    # exit() # Exiting here would stop the main script too, better to handle in main.py

# Configure the generative AI library for the EVALUATION model
# Only configure if the API key is available
if google_api_key:
    try:
        genai.configure(api_key=google_api_key)
        # Choose the Gemini model to use as the EVALUATOR
        # Use a capable model like gemini-1.5-pro-latest for best evaluation quality
        evaluation_model_name = "gemini-1.5-pro-latest"
        # evaluation_model_name = "gemini-1.0-pro" # Or an older model if preferred/required
        print(f"Using evaluation model: {evaluation_model_name}")
    except Exception as e:
        print(f"Error configuring Google Generative AI: {e}")
        print("Evaluation will not be possible.")
        google_api_key = None # Set to None to indicate configuration failed
else:
    evaluation_model_name = None # No model name if no API key
    print("GOOGLE_API_KEY not set. Evaluation functions will not work.")


# File containing your test data (prompts, expected, actual responses)
# Note: This script is now primarily for housing the evaluation functions
# The main logic for loading data and calling these functions is in main.py
# data_file = "evaluation.json" # This is now handled in main.py

# Output file for evaluation results
# results_file = "evaluation_results.json" # This is now handled in main.py

# --- Evaluation Prompt Template ---
# This template instructs the AI on how to perform the evaluation
EVALUATION_PROMPT_TEMPLATE = """
You are an automated AI response evaluator. Your task is to compare an "Actual Output" generated by another AI model against an "Expected Output" for a given "Original Prompt".

Evaluate the "Actual Output" based on whether it is a VALID response to the "Original Prompt", using the "Expected Output" as a reference for the type and quality of information expected, but not necessarily requiring an exact match.

Criteria for VALIDITY:
- Does it address the core intent of the Original Prompt?
- Is the information provided generally accurate and relevant?
- Does it provide helpful suggestions or guidance appropriate to the prompt?
- Is it clearly communicated?
- Does it avoid providing incorrect, vague, biased, or inappropriate information?

If the "Actual Output" is VALID, choose the single best reason from the following list:
- Accurate Details
- Helpful Suggestions
- Clear Communication
- General Guidance

If the "Actual Output" is INVALID, choose the single best reason from the following list:
- Incorrect Info
- Vague/Biased
- Overreaching Advice
- Poor Communication

Structure your response STRICTLY as follows:
Judgment: VALID: [Chosen Reason]
or
Judgment: INVALID: [Chosen Reason]

Do NOT include any other text, explanations, or formatting.

--- Original Prompt ---
{original_prompt}

--- Expected Output ---
{expected_output}

--- Actual Output ---
{actual_output}

--- Evaluation ---
"""

# --- Consistency Evaluation Prompt Template ---
CONSISTENCY_PROMPT_TEMPLATE = """
You are an automated AI response consistency evaluator. You will review a list of multiple responses generated by the same AI model for the same prompt.

Your task is to assess how consistent and similar these responses are to each other, focusing on the core meaning and key information conveyed, rather than minor differences in wording or structure.

Evaluate the consistency based on:
- **Similarity in Main Content:** Do the responses convey the same primary information or answer the core question in a similar way?
- **Consistency in Key Details:** Are important details (like names, dates, locations, specific facts) consistent across responses?
- **Similarity in Reasoning/Approach:** If the prompt requires reasoning or a specific approach, is that consistent?
- **Lack of Contradictory Information:** Do any responses contradict each other on important points?
- **Overall Semantic Similarity:** Do the responses mean essentially the same thing, even if phrased differently?

Provide a consistency score out of 5, where:
5: Highly consistent - Responses convey the same core meaning and key details with minor or no significant variations.
4: Mostly consistent - Responses are similar in core meaning and most key details, with some minor differences in wording or structure.
3: Moderately consistent - Responses convey similar core information but have some notable differences in details, structure, or phrasing.
2: Low consistency - Responses have significant differences in core information or key details, or some contradictory information.
1: Very low consistency - Responses are largely different, contradictory, or address the prompt in fundamentally different ways.

Structure your response STRICTLY as follows:
Consistency Score: [Numerical Score]/5. [Brief explanation]

Do NOT include any other text, explanations, or formatting before the "Consistency Score:" line.

--- Original Prompt ---
{original_prompt}

--- Expected Output ---
{expected_output}

--- Responses ---
{responses_text}

--- Consistency Evaluation ---
"""


# --- Function to Perform Evaluation API Call ---

# Modified to accept evaluation_model object directly
def get_evaluation_result(
    evaluation_model: genai.GenerativeModel,
    original_prompt: str,
    expected_output: str,
    actual_output: str,
    retries: int = 3,
    delay: int = 5
) -> str:
    """Calls the evaluation model to get a validity judgment."""
    if not evaluation_model:
        return "Error: Evaluation model not initialized."

    prompt_text = EVALUATION_PROMPT_TEMPLATE.format(
        original_prompt=original_prompt,
        expected_output=expected_output,
        actual_output=actual_output
    )

    for attempt in range(retries):
        try:
            response = evaluation_model.generate_content(prompt_text)

            # Check for block reasons or empty responses
            if hasattr(response, '_result') and response._result.prompt_feedback:
                safety_ratings = response._result.prompt_feedback.safety_ratings
                block_reason = response._result.prompt_feedback.block_reason
                if block_reason:
                    # Return a clear error message including block details
                    return f"Error: Evaluation prompt blocked. Reason: {block_reason}. Safety Ratings: {safety_ratings}"

            if hasattr(response, 'text') and response.text.strip():
                # Return the raw text response from the evaluator
                return response.text.strip()
            else:
                 # Handle cases where response.text is empty or None
                 return f"Error: Evaluation model returned empty text. Full response object: {response._result}"


        except Exception as e:
            print(
                f"  Evaluation API Error (Attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                print(f"  Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                # Return an error message if all retries fail
                return f"Error: Evaluation API call failed after {retries} attempts - {e}"
    # Should not reach here if retries > 0 and no exceptions, but as a fallback
    return "Error: Evaluation call failed after retries."


# --- Function to Perform Consistency Evaluation API Call ---
def get_consistency_score(
    evaluation_model: genai.GenerativeModel,
    original_prompt: str,
    expected_output: str,
    responses_list: List[str],
    retries: int = 3,
    delay: int = 5
) -> str:
    """Calls the evaluation model to get a consistency score."""
    if not evaluation_model:
        return "Error: Evaluation model not initialized."

    if not responses_list:
        return "Error: No responses provided for consistency evaluation."

    # Format the list of responses for the prompt
    responses_text = '\n'.join([f'- Response {i+1}: {r}' for i, r in enumerate(responses_list)])

    prompt_text = CONSISTENCY_PROMPT_TEMPLATE.format(
        original_prompt=original_prompt,
        expected_output=expected_output,
        responses_text=responses_text
    )

    for attempt in range(retries):
        try:
            response = evaluation_model.generate_content(prompt_text)

            # Check for block reasons or empty responses
            if hasattr(response, '_result') and response._result.prompt_feedback:
                safety_ratings = response._result.prompt_feedback.safety_ratings
                block_reason = response._result.prompt_feedback.block_reason
                if block_reason:
                     # Return a clear error message including block details
                    return f"Error: Consistency prompt blocked. Reason: {block_reason}. Safety Ratings: {safety_ratings}"

            if hasattr(response, 'text') and response.text.strip():
                # Return the raw text response from the evaluator
                return response.text.strip()
            else:
                # Handle cases where response.text is empty or None
                return f"Error: Consistency evaluation model returned empty text. Full response object: {response._result}"


        except Exception as e:
            print(
                f"  Consistency API Error (Attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                print(f"  Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                 # Return an error message if all retries fail
                return f"Error: Consistency API call failed after {retries} attempts - {e}"
    # Should not reach here
    return "Error: Consistency call failed after retries."


# --- Function to Parse Pass/Fail Result ---
def parse_pass_fail(evaluation_output: str) -> bool:
    """
    Parses the output of get_evaluation_result to determine if it passed.
    Assumes the output string contains "VALID" or "INVALID" following a 'Judgment:' marker.
    Returns True for VALID, False for INVALID or parsing errors.
    """
    if not isinstance(evaluation_output, str):
        return False # Cannot parse non-string output

    evaluation_output_lower = evaluation_output.lower()

    # Look for the marker "judgment:" and then "valid" or "invalid"
    marker = "judgment:"
    if marker in evaluation_output_lower:
        part_after_marker = evaluation_output_lower.split(marker, 1)[1].strip() # Use strip() to remove leading/trailing whitespace
        if part_after_marker.startswith("valid"):
            return True
        elif part_after_marker.startswith("invalid"):
            return False

    # Fallback if marker not found or format is unexpected
    # Try to find "valid" or "invalid" anywhere if marker parsing fails
    if "valid" in evaluation_output_lower and "invalid" not in evaluation_output_lower:
         return True
    elif "invalid" in evaluation_output_lower and "valid" not in evaluation_output_lower:
         return False
    elif "valid" in evaluation_output_lower and "invalid" in evaluation_output_lower:
         # Ambiguous case, maybe log warning
         print(f"Warning: Ambiguous valid/invalid in evaluation output: {evaluation_output[:100]}...")
         return False # Default to False

    # If neither "valid" nor "invalid" is clearly found
    # Check if it's an error message from the API call itself
    if evaluation_output.startswith("Error:"):
         print(f"Warning: Evaluation output is an error message: {evaluation_output[:100]}...")
         return False # Treat API errors as failures

    print(f"Warning: Could not parse valid/invalid from evaluation output: {evaluation_output[:100]}...")
    return False


# --- Function to Parse Numerical Score from Evaluation ---
def parse_evaluation_score(evaluation_output: str) -> float | None:
    """
    Parses the output of get_consistency_score to extract a numerical score (e.g., from "X/5").
    Assumes the output string contains a score like "X/5" following a 'Consistency Score:' marker
    or a leading number.
    Returns the numerical score (as a float) or None if parsing fails.
    """
    if not isinstance(evaluation_output, str):
        return None # Cannot parse non-string output

    evaluation_output_lower = evaluation_output.lower()

    # Look for the marker "consistency score:"
    marker = "consistency score:"
    if marker in evaluation_output_lower:
        try:
            # Extract the part after the marker
            start_index = evaluation_output_lower.find(marker) + len(marker)
            score_part = evaluation_output[start_index:].strip()

            # Try to parse "X/5" format
            if '/' in score_part:
                numerator_str = score_part.split('/')[0].strip()
                # Attempt to convert the numerator to a float
                return float(numerator_str)
            else:
                # If not X/5, try to parse a leading number
                 try:
                    # Take the first segment of text and try to convert to float
                    leading_text = score_part.split(maxsplit=1)[0]
                    return float(leading_text)
                 except (ValueError, IndexError):
                    pass # Parsing leading number failed

        except Exception as e:
             print(f"Warning: Error parsing evaluation score after marker: {evaluation_output[:100]}... Error: {e}")
             return None # Parsing failed

    # Fallback: If marker not found, try to parse a leading number from the whole string
    try:
        leading_text = evaluation_output.split(maxsplit=1)[0]
        return float(leading_text)
    except (ValueError, IndexError):
        pass # Parsing leading number failed from the start

    # If no known format is found and leading number parsing failed
    # Check if it's an error message from the API call itself
    if evaluation_output.startswith("Error:"):
         print(f"Warning: Consistency evaluation output is an error message: {evaluation_output[:100]}...")
         return None # Treat API errors as unparsable scores

    print(f"Warning: Could not parse numerical score from evaluation output: {evaluation_output[:100]}...")
    return None # Return None if parsing fails


# --- Main Evaluation Logic ---
# This function is likely not needed anymore as main.py orchestrates the process
# Keeping it here for reference, but it won't be called by main.py in the new structure
def run_evaluation():
    """Loads data, performs evaluations, and saves results."""
    print("Note: run_evaluation in eva_res.py is typically not called directly when using main.py")
    # The logic for loading data, initializing models, and saving results
    # has been moved to main.py to handle the multi-model and retry logic.

    # Example of how you would use the functions if run independently:
    # try:
    #     evaluator_model = genai.GenerativeModel(model_name=evaluation_model_name)
    # except Exception as e:
    #      print(f"Error initializing evaluation model: {e}")
    #      return

    # # Example usage (assuming you have actual responses loaded)
    # prompt = "What is the capital of France?"
    # expected = "Paris"
    # actual_single = "Paris is the capital."
    # actual_multiple = ["Paris is the capital.", "The capital of France is Paris.", "Paris."]

    # # Example of accuracy evaluation
    # accuracy_eval = get_evaluation_result(evaluator_model, prompt, expected, actual_single)
    # print(f"Accuracy Evaluation: {accuracy_eval}")
    # print(f"Parsed Pass/Fail: {parse_pass_fail(accuracy_eval)}")

    # # Example of consistency evaluation
    # if len(actual_multiple) > 1:
    #     consistency_eval = get_consistency_score(evaluator_model, prompt, expected, actual_multiple)
    #     print(f"Consistency Evaluation: {consistency_eval}")
    #     print(f"Parsed Score: {parse_evaluation_score(consistency_eval)}")


# The __main__ block is also typically handled by main.py now
# if __name__ == "__main__":
#     run_evaluation()
