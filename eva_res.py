

# pip install google-generativeai  python-dotenv
from typing import List, Dict, Any, Optional
import time
import json
import google.generativeai as genai
import os
from dotenv import load_dotenv  # Import the library

load_dotenv()  # Load variables from .env file

# --- Configuration ---


google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    print("Error: GOOGLE_API_KEY environment variable not set.")
    print("Please set it before running the script.")
    exit()  # Exit if the API key is not found

# Configure the generative AI library for the EVALUATION model
genai.configure(api_key=google_api_key)

# Choose the Gemini model to use as the EVALUATOR
# Use a capable model like gemini-1.5-pro-latest for best evaluation quality
evaluation_model_name = "gemini-1.5-pro-latest"
# evaluation_model_name = "gemini-1.0-pro" # Or an older model if preferred/required

print(f"Using evaluation model: {evaluation_model_name}")

# File containing your test data (prompts, expected, actual responses)
# Make sure this file exists and is correctly formatted
data_file = "evaluation.json"

# Output file for evaluation results
results_file = "evaluation_results.json"

# --- Evaluation Prompt Template ---
# This template instructs the AI on how to perform the evaluation
EVALUATION_PROMPT_TEMPLATE = """
You are an automated AI response evaluator. Your task is to compare an "Actual Output" generated by another AI model against an "Expected Output" for a given "Original Prompt".

Evaluate the "Actual Output" based on whether it is a VALID response to the "Original Prompt", using the "Expected Output" as a reference for the type and quality of information expected, but not necessarily requiring an exact match.

Criteria for VALIDITY:
- Does it address the core intent of the Original Prompt?
- Is the information provided generally accurate and relevant?
- Does it provide helpful suggestions or guidance appropriate to the prompt?
- Is it clearly communicated?
- Does it avoid providing incorrect, vague, biased, or inappropriate information?

If the "Actual Output" is VALID, choose the single best reason from the following list:
- Accurate Details
- Helpful Suggestions
- Clear Communication
- General Guidance

If the "Actual Output" is INVALID, choose the single best reason from the following list:
- Incorrect Info
- Vague/Biased
- Overreaching Advice
- Poor Communication

Structure your response STRICTLY as follows:
VALID: [Chosen Reason]
or
INVALID: [Chosen Reason]

Do NOT include any other text, explanations, or formatting.

--- Original Prompt ---
{original_prompt}

--- Expected Output ---
{expected_output}

--- Actual Output ---
{actual_output}

--- Evaluation ---
"""

# --- Function to Perform Evaluation API Call ---


def get_evaluation_result(
    evaluation_model: genai.GenerativeModel,
    original_prompt: str,
    expected_output: str,
    actual_output: str,
    retries: int = 3,
    delay: int = 5
) -> str:
    """Calls the evaluation model to get a validity judgment."""
    prompt_text = EVALUATION_PROMPT_TEMPLATE.format(
        original_prompt=original_prompt,
        expected_output=expected_output,
        actual_output=actual_output
    )

    for attempt in range(retries):
        try:
            response = evaluation_model.generate_content(prompt_text)

            # Check for block reasons or empty responses
            if response._result.prompt_feedback:
                safety_ratings = response._result.prompt_feedback.safety_ratings
                block_reason = response._result.prompt_feedback.block_reason
                if block_reason:
                    return f"Error: Evaluation prompt blocked. Reason: {block_reason}. Safety Ratings: {safety_ratings}"

            if hasattr(response, 'text') and response.text.strip():
                # Return the raw text response from the evaluator
                return response.text.strip()
            else:
                return f"Error: Evaluation model returned no text. Full response object: {response._result}"

        except Exception as e:
            print(
                f"  Evaluation API Error (Attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                print(f"  Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                return f"Error: Evaluation API call failed after {retries} attempts - {e}"
    return "Error: Evaluation call failed after retries."


# --- Main Evaluation Logic ---
def run_evaluation():
    """Loads data, performs evaluations, and saves results."""
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Data file not found at {data_file}")
        print("Please create 'evaluation_data.json' with your test prompts, expected, and actual outputs.")
        return
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {data_file}")
        print("Please check the file formatting.")
        return
    except Exception as e:
        print(f"Error loading data file: {e}")
        return

    if not test_data:
        print(f"Error: No data found in {data_file}")
        return

    # Initialize the evaluation model
    try:
        evaluator_model = genai.GenerativeModel(
            model_name=evaluation_model_name)
    except Exception as e:
        print(
            f"Error initializing evaluation model '{evaluation_model_name}': {e}")
        print("Please check the model name and your API key.")
        return

    evaluation_results: List[Dict[str, Any]] = []

    print(
        f"\n--- Starting Automated Response Evaluation ({len(test_data)} inputs) ---")
    print(f"Using evaluator model: {evaluation_model_name}")

    for i, test_case in enumerate(test_data):
        prompt = test_case.get("prompt")
        expected = test_case.get("expected_output")
        actual_chatgpt = test_case.get("actual_output_chatgpt")
        actual_deepseek = test_case.get("actual_output_deepseek")

        if not all([prompt, expected, actual_chatgpt is not None, actual_deepseek is not None]):
            print(f"Skipping test case {i+1} due to missing data: {test_case}")
            continue

        print(f"\n--- Evaluating Input {i + 1}/{len(test_data)} ---")
        print(f"Prompt: {prompt[:80]}...")  # Print truncated prompt
        # print(f"  Expected: {expected[:80]}...") # Optional: print truncated expected
        # print(f"  Actual (ChatGPT): {actual_chatgpt[:80]}...") # Optional: print truncated actual
        # print(f"  Actual (DeepSeek): {actual_deepseek[:80]}...") # Optional: print truncated actual

        # Evaluate ChatGPT's response
        print("  Evaluating ChatGPT response...")
        chatgpt_evaluation_raw = get_evaluation_result(
            evaluator_model, prompt, expected, actual_chatgpt
        )
        print(f"    ChatGPT Evaluation Result: {chatgpt_evaluation_raw}")

        # Evaluate DeepSeek's response
        print("  Evaluating DeepSeek response...")
        deepseek_evaluation_raw = get_evaluation_result(
            evaluator_model, prompt, expected, actual_deepseek
        )
        print(f"    DeepSeek Evaluation Result: {deepseek_evaluation_raw}")

        # Store the results (can parse the raw evaluation string if needed)
        evaluation_results.append({
            "prompt": prompt,
            "expected_output": expected,
            "actual_output_chatgpt": actual_chatgpt,
            "actual_output_deepseek": actual_deepseek,
            "evaluation_chatgpt": chatgpt_evaluation_raw,
            "evaluation_deepseek": deepseek_evaluation_raw,
        })

        # Add a small delay between evaluation calls
        time.sleep(1)

    # --- Save Evaluation Results ---
    try:
        with open(results_file, "w", encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=4, ensure_ascii=False)
        print(f"\nAll evaluation results saved to {results_file}")
    except Exception as e:
        print(
            f"\nCould not save evaluation results to file {results_file}: {e}")

    print("\n--- Evaluation Complete ---")


if __name__ == "__main__":
    run_evaluation()
