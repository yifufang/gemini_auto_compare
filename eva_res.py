from typing import List, Dict, Any, Optional
import time
import json
import google.generativeai as genai
import os
from dotenv import load_dotenv

load_dotenv()

google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    print("Error: GOOGLE_API_KEY environment variable not set.")
    print("Please set it before running the script.")
    print("Evaluation will not be possible.")

if google_api_key:
    try:
        genai.configure(api_key=google_api_key)
        evaluation_model_name = "gemini-1.5-pro-latest"
        print(f"Using evaluation model: {evaluation_model_name}")
    except Exception as e:
        print(f"Error configuring Google Generative AI: {e}")
        print("Evaluation will not be possible.")
        google_api_key = None
else:
    evaluation_model_name = None
    print("GOOGLE_API_KEY not set. Evaluation functions will not work.")

EVALUATION_PROMPT_TEMPLATE = """
You are an automated AI response evaluator. Your task is to compare an "Actual Output" generated by another AI model against an "Expected Output" for a given "Original Prompt".

Evaluate the "Actual Output" based on whether it is a VALID response to the "Original Prompt", using the "Expected Output" as a reference for the type and quality of information expected, but not necessarily requiring an exact match.

Criteria for VALIDITY:
- Does it address the core intent of the Original Prompt?
- Is the information provided generally accurate and relevant?
- Does it provide helpful suggestions or guidance appropriate to the prompt?
- Is it clearly communicated?
- Does it avoid providing incorrect, vague, biased, or inappropriate information?

If the "Actual Output" is VALID, choose the single best reason from the following list:
- Accurate Details
- Helpful Suggestions
- Clear Communication
- General Guidance

If the "Actual Output" is INVALID, choose the single best reason from the following list:
- Incorrect Info
- Vague/Biased
- Overreaching Advice
- Poor Communication

Structure your response STRICTLY as follows:
Judgment: VALID: [Chosen Reason]
or
Judgment: INVALID: [Chosen Reason]

Do NOT include any other text, explanations, or formatting.

--- Original Prompt ---
{original_prompt}

--- Expected Output ---
{expected_output}

--- Actual Output ---
{actual_output}

--- Evaluation ---
"""

CONSISTENCY_PROMPT_TEMPLATE = """
You are an automated AI response consistency evaluator. You will review a list of multiple responses generated by the same AI model for the same prompt.

Your task is to assess how consistent and similar these responses are to each other, focusing on the core meaning and key information conveyed, rather than minor differences in wording or structure.

Evaluate the consistency based on:
- **Similarity in Main Content:** Do the responses convey the same primary information or answer the core question in a similar way?
- **Consistency in Key Details:** Are important details (like names, dates, locations, specific facts) consistent across responses?
- **Similarity in Reasoning/Approach:** If the prompt requires reasoning or a specific approach, is that consistent?
- **Lack of Contradictory Information:** Do any responses contradict each other on important points?
- **Overall Semantic Similarity:** Do the responses mean essentially the same thing, even if phrased differently?

Provide a consistency score out of 5, where:
5: Highly consistent - Responses convey the same core meaning and key details with minor or no significant variations.
4: Mostly consistent - Responses are similar in core meaning and most key details, with some minor differences in wording or structure.
3: Moderately consistent - Responses convey similar core information but have some notable differences in details, structure, or phrasing.
2: Low consistency - Responses have significant differences in core information or key details, or some contradictory information.
1: Very low consistency - Responses are largely different, contradictory, or address the prompt in fundamentally different ways.

Structure your response STRICTLY as follows:
Consistency Score: [Numerical Score]/5. [Brief explanation]

Do NOT include any other text, explanations, or formatting before the "Consistency Score:" line.

--- Original Prompt ---
{original_prompt}

--- Expected Output ---
{expected_output}

--- Responses ---
{responses_text}

--- Consistency Evaluation ---
"""

def get_evaluation_result(
    evaluation_model: genai.GenerativeModel,
    original_prompt: str,
    expected_output: str,
    actual_output: str,
    retries: int = 3,
    delay: int = 5
) -> str:
    if not evaluation_model:
        return "Error: Evaluation model not initialized."

    prompt_text = EVALUATION_PROMPT_TEMPLATE.format(
        original_prompt=original_prompt,
        expected_output=expected_output,
        actual_output=actual_output
    )

    for attempt in range(retries):
        try:
            response = evaluation_model.generate_content(prompt_text)

            if hasattr(response, '_result') and response._result.prompt_feedback:
                safety_ratings = response._result.prompt_feedback.safety_ratings
                block_reason = response._result.prompt_feedback.block_reason
                if block_reason:
                    return f"Error: Evaluation prompt blocked. Reason: {block_reason}. Safety Ratings: {safety_ratings}"

            if hasattr(response, 'text') and response.text.strip():
                return response.text.strip()
            else:
                 return f"Error: Evaluation model returned empty text. Full response object: {response._result}"

        except Exception as e:
            print(
                f"  Evaluation API Error (Attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                print(f"  Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                return f"Error: Evaluation API call failed after {retries} attempts - {e}"
    return "Error: Evaluation call failed after retries."

def get_consistency_score(
    evaluation_model: genai.GenerativeModel,
    original_prompt: str,
    expected_output: str,
    responses_list: List[str],
    retries: int = 3,
    delay: int = 5
) -> str:
    if not evaluation_model:
        return "Error: Evaluation model not initialized."

    if not responses_list:
        return "Error: No responses provided for consistency evaluation."

    responses_text = '\n'.join([f'- Response {i+1}: {r}' for i, r in enumerate(responses_list)])

    prompt_text = CONSISTENCY_PROMPT_TEMPLATE.format(
        original_prompt=original_prompt,
        expected_output=expected_output,
        responses_text=responses_text
    )

    for attempt in range(retries):
        try:
            response = evaluation_model.generate_content(prompt_text)

            if hasattr(response, '_result') and response._result.prompt_feedback:
                safety_ratings = response._result.prompt_feedback.safety_ratings
                block_reason = response._result.prompt_feedback.block_reason
                if block_reason:
                    return f"Error: Consistency prompt blocked. Reason: {block_reason}. Safety Ratings: {safety_ratings}"

            if hasattr(response, 'text') and response.text.strip():
                return response.text.strip()
            else:
                return f"Error: Consistency evaluation model returned empty text. Full response object: {response._result}"

        except Exception as e:
            print(
                f"  Consistency API Error (Attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                print(f"  Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                 return f"Error: Consistency API call failed after {retries} attempts - {e}"
    return "Error: Consistency call failed after retries."

def parse_pass_fail(evaluation_output: str) -> bool:
    if not isinstance(evaluation_output, str):
        return False

    evaluation_output_lower = evaluation_output.lower()

    marker = "judgment:"
    if marker in evaluation_output_lower:
        part_after_marker = evaluation_output_lower.split(marker, 1)[1].strip()
        if part_after_marker.startswith("valid"):
            return True
        elif part_after_marker.startswith("invalid"):
            return False

    if "valid" in evaluation_output_lower and "invalid" not in evaluation_output_lower:
         return True
    elif "invalid" in evaluation_output_lower and "valid" not in evaluation_output_lower:
         return False
    elif "valid" in evaluation_output_lower and "invalid" in evaluation_output_lower:
         print(f"Warning: Ambiguous valid/invalid in evaluation output: {evaluation_output[:100]}...")
         return False

    if evaluation_output.startswith("Error:"):
         print(f"Warning: Evaluation output is an error message: {evaluation_output[:100]}...")
         return False

    print(f"Warning: Could not parse valid/invalid from evaluation output: {evaluation_output[:100]}...")
    return False

def parse_evaluation_score(evaluation_output: str) -> float | None:
    if not isinstance(evaluation_output, str):
        return None

    evaluation_output_lower = evaluation_output.lower()

    marker = "consistency score:"
    if marker in evaluation_output_lower:
        try:
            start_index = evaluation_output_lower.find(marker) + len(marker)
            score_part = evaluation_output[start_index:].strip()

            if '/' in score_part:
                numerator_str = score_part.split('/')[0].strip()
                return float(numerator_str)
            else:
                 try:
                    leading_text = score_part.split(maxsplit=1)[0]
                    return float(leading_text)
                 except (ValueError, IndexError):
                    pass

        except Exception as e:
             print(f"Warning: Error parsing evaluation score after marker: {evaluation_output[:100]}... Error: {e}")
             return None

    try:
        leading_text = evaluation_output.split(maxsplit=1)[0]
        return float(leading_text)
    except (ValueError, IndexError):
        pass

    if evaluation_output.startswith("Error:"):
         print(f"Warning: Consistency evaluation output is an error message: {evaluation_output[:100]}...")
         return None

    print(f"Warning: Could not parse numerical score from evaluation output: {evaluation_output[:100]}...")
    return None


def run_evaluation():
    print("Note: run_evaluation in eva_res.py is typically not called directly when using main.py")


if __name__ == "__main__":
    run_evaluation()